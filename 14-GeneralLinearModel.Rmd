---
output:
  pdf_document: default
  bookdown::gitbook:
    lib_dir: "book_assets"
    includes:
      in_header: google_analytics.html
  html_document: default
---
<!-- # The General Linear Model -->
# El Modelo Lineal General {#the-general-lineal-model}

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(fivethirtyeight)
library(caret)
library(MASS)
library(cowplot)

library(knitr)

set.seed(123456) # set random seed to exactly replicate results
opts_chunk$set(tidy.opts=list(width.cutoff=80))
options(tibble.width = 60)

# load the NHANES data library
library(NHANES)

# drop duplicated IDs within the NHANES dataset
NHANES <- 
  NHANES %>% 
  dplyr::distinct(ID,.keep_all=TRUE)

NHANES_adult <- 
  NHANES %>%
  drop_na(Weight) %>%
  subset(Age>=18)


```

<!-- Remember that early in the book we described the basic model of statistics: -->
Recuerda que previamente en este libro describimos el modelo básico en estadística:

<!-- data = model + error -->
$$
datos = modelo + error
$$
<!-- where our general goal is to find the model that minimizes the error, subject to some other constraints (such as keeping the model relatively simple so that we can generalize beyond our specific dataset). In this chapter we will focus on a particular implementation of this approach, which is known as the *general linear model* (or GLM).   You have already seen the general linear model in the earlier chapter on Fitting Models to Data, where we modeled height in the NHANES dataset as a function of age; here we will provide a more general introduction to the concept of the GLM and its many uses.  Nearly every model used in statistics can be framed in terms of the general linear model or an extension of it. -->
donde nuestro objetivo principal es encontrar el modelo que minimice el error, sujeto a otras restricciones (como el mantener el modelo relativamente simple para que podamos generalizar más allá de nuestros datos específicos). En este capítulo nos enfocaremos en una implementación particular de esta aproximación, que es conocido como el *modelo lineal general* (*general linear model*, o GLM).    Ya has visto el modelo lineal general en el capítulo de Ajustar Modelos a los Datos, donde modelamos la altura de los datos de la base NHANES como una función de la edad; aquí proveeremos una introducción más general al concepto del GLM y de sus múltiples usos. Casi todos los modelos usados en estadística pueden ser planteados en términos del modelo lineal general o como una extensión de éste.

<!-- Before we discuss the general linear model, let's first define two terms that will be important for our discussion: -->
Antes de que discutamos el modelo lineal general, primero definamos dos conceptos que serán importantes para nuestra discusión:

<!-- - *dependent variable*: This is the outcome variable that our model aims to explain (usually referred to as *Y*) -->
<!-- - *independent variable*: This is a variable that we wish to use in order to explain the dependent variable (usually referred to as *X*).   -->
- *variable dependiente*: Es la variable de los resultados que nuestro modelo busca explicar (usualmente referida como *Y*)
- *variable independiente*: Es la variable que queremos usar para poder explicar la variable dependiente (usualmente referida como *X*)

<!-- There may be multiple independent variables, but for this course we will focus primarily on situations where there is only one dependent variable in our analysis. -->
Puede haber múltiples variables dependientes, pero para este curso nos enfocaremos principalmente en situaciones donde sólo hay una variable dependiente en nuestro análisis.

<!-- A general linear model is one in which the model for the dependent variable is composed of a *linear combination* of independent variables that are each multiplied by a weight (which is often referred to as the Greek letter beta - $\beta$), which determines the relative contribution of that independent variable to the model prediction. -->
Un modelo lineal general es uno en donde nuestro modelo para la variable dependiente está compuesto por una *combinación lineal* de variables independientes que cada una es multiplicada por un peso (el cual es frecuentemente referido con la letra griega beta - $\beta$), que determina la contribución relativa de esa variable independiente a la predicción del modelo.

```{r echo=FALSE}
# create simulated data for example
set.seed(12345)

# the number of points that having a prior class increases grades
betas <- c(6, 5)

df <-
  tibble(
    studyTime = c(2, 3, 5, 6, 6, 8, 10, 12) / 3,
    priorClass = c(0, 1, 1, 0, 1, 0, 1, 0)
  ) %>%
  mutate(
    grade = 
      studyTime * betas[1] + 
      priorClass * betas[2] + 
      round(rnorm(8, mean = 70, sd = 5))
  )
```

<!-- Relation between study time and grades -->
```{r StudytimeGrades, echo=FALSE,fig.cap='Relación entre tiempo de estudio y calificaciones.',fig.width=3,fig.height=3,out.height='50%'}
p <- ggplot(df,aes(studyTime,grade)) +
  geom_point(size=3) +
  xlab('Study time (hours)') +
  ylab('Grade (percent)') +
  xlim(0,5) + 
  ylim(70,100)

print(p)
```


<!-- As an example, let's generate some simulated data for the relationship between study time and exam grades (see Figure \@ref(fig:StudytimeGrades)). Given these data, we might want to engage in each of the three fundamental activities of statistics: -->
Como ejemplo, generemos unos datos simulados de la relación entre tiempo de estudio y calificación en exámenes (ve la Figura \@ref(fig:StudytimeGrades)). Dados estos datos, quisiéramos hacer el ejercicio de desarrollar cada una de las tres actividades fundamentales de la Estadística:

<!-- - *Describe*: How strong is the relationship between grade and study time? -->
<!-- - *Decide*: Is there a statistically significant relationship between grade and study time? -->
<!-- - *Predict*: Given a particular amount of study time, what grade do we expect? -->
- *Describir*: ¿Qué tan fuerte es la relación entre calificación y tiempo de estudio?
- *Decidir*: ¿Hay una relación estadísticamente significativa entre calificación y tiempo de estudio?
- *Predecir*: Dado un tiempo particular de estudio, ¿qué calificación esperaríamos?

<!-- In the previous chapter we learned how to describe the relationship between two variables using the correlation coefficient.  Let's use our statistical software to compute that relationship for these data and test whether the correlation is significantly different from zero: -->
En el capítulo anterior aprendimos cómo describir la relación entre dos variables usando el coeficiente de correlación. Usemos nuestro software estadístico para calcular esa relación para estos datos y hacer la prueba de si esa correlación es significativamente diferente de cero:

```{r echo=FALSE}
# compute correlation between grades and study time
corTestResult <- cor.test(df$grade, df$studyTime)
corTestResult
```

<!-- The correlation is quite high, but notice that the confidence interval around the estimate is very wide, spanning nearly the entire range from zero to one, which is due in part to the small sample size.   -->
Esta correlación es bastante alta, pero nota que el intervalo de confianza alrededor de nuestra estimación es bastante amplio, cubriendo casi todo el rango de valores desde cero a uno, que en parte se debe a que nuestro tamaño de muestra es pequeño.

<!-- ## Linear regression {#linear-regression} -->
## Regresión lineal {#linear-regression}

<!-- We can use the general linear model to describe the relation between two variables and to decide whether that relationship is statistically significant; in addition, the model allows us to predict the value of the dependent variable given some new value(s) of the independent variable(s).  Most importantly, the general linear model will allow us to build models that incorporate multiple independent variables, whereas the correlation coefficient can only describe the relationship between two individual variables. -->
Podemos usar el modelo lineal general para describir la relación entre dos variables y para decidir si la relación es estadísticamente significativa; además, el modelo nos permite predecir el valor de la variable dependiente dado algún o algunos valores nuevos de la(s) variable(s) independiente(s). Aún más importante, el modelo lineal general nos permite construir modelos que incorporen múltiples variables independientes, mientras que el coeficiente de correlación sólo puede describir la relación entre dos variables individuales.

<!-- The specific version of the GLM that we use for this is referred to as as *linear regression*.  The term *regression* was coined by Francis Galton, who had noted that when he compared parents and their children on some feature (such as height), the children of extreme parents (i.e. the very tall or very short parents) generally fell closer to the mean than did their parents.  This is an extremely important point that we return to below. -->
La versión específica del GLM que usamos para esto es conocida como *regresión lineal* (*linear regression*). El término *regresión* fue acuñado por Francis Galton, quien notó que cuando él comparó padres y madres con sus hijxs en algunas características (como altura), lxs hijxs de padres y madres con valores extremos (i.e. lxs padres y madres muy altxs, o muy bajxs de estatura) generalmente caían más cerca de la media que de los valores de sus padres y madres. Este es un punto extremadamente importante al que regresaremos más abajo.

<!-- The simplest version of the linear regression model (with a single independent variable) can be expressed as follows: -->
La versión más simple del modelo de regresión lineal (con una sola variable independiente) puede ser expresada de la siguiente manera:

$$
y = x * \beta_x + \beta_0 + \epsilon
$$
<!-- The $\beta_x$ value tells us how much we would expect y to change given a one-unit change in $x$.  The intercept $\beta_0$ is an overall offset, which tells us what value we would expect y to have when $x=0$; you may remember from our early modeling discussion that this is important to model the overall magnitude of the data, even if $x$ never actually attains a value of zero. The error term $\epsilon$ refers to whatever is left over once the model has been fit; we often refer to these as the *residuals* from the model. If we want to know how to predict y (which we call $\hat{y}$) after we estimate the $\beta$ values, then we can drop the error term: -->
El valor $\beta_x$  nos dice cuánto esperaríamos que $y$ cambiara dado un cambio de una sola unidad en $x$. La constante $\beta_0$ es un *offset* general (una compensación general), que nos dice cuál valor esperaríamos de $y$ cuando $x=0$; recordarás de nuestra discusión previa sobre modelos que esto es importante para poder modelar la magnitud general de los datos, aún cuando $x$ nunca tenga realmente un valor de cero. El término del error $\epsilon$ se refiere al error que queda después de que el modelo ha sido ajustado a los datos; frecuentemente nos referimos a este error como los *residuales* del modelo. Si queremos saber cómo predecir $y$ (que llamamos $\hat{y}$) después de estimar los valores $\beta$, entonces podemos ignorar el término del error por el momento:

$$
\hat{y} = x * \hat{\beta_x} + \hat{\beta_0} 
$$
<!-- Note that this is simply the equation for a line, where $\hat{\beta_x}$ is our estimate of the slope and $\beta_0$ is the intercept. Figure  \@ref(fig:LinearRegression) shows an example of this model applied to the study time data. -->
Nota que esto es simplemente la ecuación de una línea, donde $\hat{\beta_x}$ es nuestra estimación de la pendiente y $\hat{\beta_0}$ es la constante. La Figura \@ref(fig:LinearRegression) muestra un ejemplo de este modelo aplicado a los datos de tiempo de estudio.

<!-- The linear regression solution for the study time data is shown in the solid line The value of the intercept is equivalent to the predicted value of the y variable when the x variable is equal to zero; this is shown with a dotted line.  The value of beta is equal to the slope of the line -- that is, how much y changes for a unit change in x.  This is shown schematically in the dashed lines, which show the degree of increase in grade for a single unit increase in study time. -->
```{r LinearRegression,echo=FALSE,fig.cap="La solución de la regresión lineal para los datos de tiempo de estudio se muestran con la línea continua azul. El valor de la constante es equivalente al valor predicho en la variable Y cuando la variable X es igual a cero; esto se muestra con la línea punteada. El valor de beta es igual a la pendiente de la línea -- esto es, cuánto cambia Y por cada unidad de cambio en X. Esto se muestra esquemáticamente con las líneas discontinuas (que forman un triángulo con la línea azul), las cuales muestran el nivel de incremento en calificación por cada unidad incrementada en tiempo de estudio.",fig.width=4,fig.height=4,out.height='50%'}

lmResult <- lm(grade~studyTime,data=df)

p2 <- p+geom_abline(slope=lmResult$coefficients[2],
                  intercept=lmResult$coefficients[1],
                  color='blue')


p3 <- p2 +
  geom_hline(yintercept=lmResult$coefficients[1],color='black',size=0.5,linetype='dotted') +
  annotate('segment',x=2,xend=3,color='red',linetype='dashed',
           y=predict(lmResult,newdata=data.frame(studyTime=2))[1],
           yend=predict(lmResult,newdata=data.frame(studyTime=2))[1]) +
   annotate('segment',x=3,xend=3,color='red',linetype='dashed',
           y=predict(lmResult,newdata=data.frame(studyTime=2))[1],
           yend=predict(lmResult,newdata=data.frame(studyTime=3))[1])
 
print(p3)

```

<!-- We will not go into the details of how the best fitting slope and intercept are actually estimated from the data; if you are interested, details are available in the Appendix. -->
No entraremos en detalles sobre cómo se calcula la pendiente y la constante que mejor ajustan a los datos; si estás interesadx, los detalles están disponibles en el Apéndice.

<!-- ### Regression to the mean {#regression-to-the-mean} -->
### Regresión a la media {#regression-to-the-mean}

<!-- The concept of *regression to the mean* was one of Galton's essential contributions to science, and it remains a critical point to understand when we interpret the results of experimental data analyses.  Let's say that we want to study the effects of a reading intervention on the performance of poor readers.  To test our hypothesis, we might go into a school and recruit those individuals in the bottom 25% of the distribution on some reading test, administer the intervention, and then examine their performance on the test after the intervention.  Let's say that the intervention actually has no effect, such that reading scores for each individual are simply independent samples from a normal distribution.  Results from a computer simulation of this hypothetic experiment are presented in \@ref(tab:readingTable). -->
El concepto de *regresión a la media* fue una de las contribuciones esenciales de Galton a la ciencia, y aún se mantiene como un punto crítico a entender cuando interpretamos los resultados de análisis de datos experimentales. Digamos que queremos estudiar los efectos de una intervención de lectura sobre el rendimiento de lectores con baja habilidad de lectura ("malos lectores"). Para probar nuestra hipótesis, podríamos ir a una escuela y reclutar a aquellas personas en el 25% más bajo de la distribución en una prueba de lectura, administrar la intervención, y luego examinar su rendimiento en la prueba después de la intervención. Digamos que la intervención realmente no tiene efecto, de manera que los puntajes de lectura de cada persona son simplemente muestras independientes de una distribución normal. Los resultados de una simulación por computadora de este experimento hipotético se presentan en la Tabla \@ref(tab:readingTable).

```{r readingTable, echo=FALSE}
# create simulated data for regression to the mean example

nstudents <- 100 

readingScores <- data.frame(
  #random normal distribution of scores for test 1
  test1 = rnorm(n = nstudents, mean = 0, sd = 1) * 10 + 100, 
  #random normal distribution of scores for test 2
  test2 = rnorm(n = nstudents, mean = 0, sd = 1) * 10 + 100 
)

# select the students in the bottom 25% on the first test
cutoff <- quantile(readingScores$test1, 0.25)

readingScores <-
  readingScores %>%
  mutate(badTest1 = test1 < cutoff) %>% 
  dplyr::filter(badTest1 == TRUE) %>%
  summarize(
    `Test 1` = mean(test1),
    `Test 2` = mean(test2)
  )
readingScores = as.data.frame(t(readingScores)) %>% rename(Score = V1)
# Reading scores for Test 1 (which is lower, because it was the basis for selecting the students) and Test 2 (which is higher because it was not related to Test 1).
kable(readingScores, caption='Puntajes de Lectura del Test 1 (que son menores, porque fueron la base para seleccionar a lxs estudiantes) y del Test 2 (que son mayores, porque no se relacionaban con el Test 1).')
```

<!-- If we look at the difference between the mean test performance at the first and second test, it appears that the intervention has helped these students substantially, as their scores have gone up by more than ten points on the test!  However, we know that in fact the students didn't improve at all, since in both cases the scores were simply selected from a random normal distribution. What has happened is that some students scored badly on the first test simply due to random chance. If we select just those subjects on the basis of their first test scores, they are guaranteed to move back towards the mean of the entire group on the second test, even if there is no effect of training. This is the reason that we always need an untreated *control group* in order to interpret any changes in performance due to an intervention; otherwise we are likely to be tricked by regression to the mean.  In addition, the participants need to be randomly assigned to the control or treatment group, so that there won't be any systematic differences between the groups (on average). -->
Si observamos la diferencia entre el rendimiento promedio en el primer y en el segundo test, pareciera que la intervención ha ayudado a estos estudiantes sustancialmente, ¡pues sus puntajes han incrementado en más de diez puntos en la prueba! Sin embargo, sabemos de cierto que estos estudiantes no mejoraron para nada, pues en ambos casos los puntajes simplemente fueron seleccionados de una distribución normal aleatoria. Lo que ha sucedido es que algunos estudiantes obtuvieron puntajes bajos en el primer test simplemente debido al azar. Si seleccionamos justo esos estudiantes con base en su puntaje del primer test, está garantizado que sus puntajes promedio se moverán hacia la media del grupo completo en su segundo test, aún cuando no hay ningún efecto del entrenamiento. Esta es una de las razones por las que siempre necesitamos un *grupo control* al que no se haya aplicado la intervención, para poder estar en posición de interpretar cualquier cambio en rendimiento como un cambio debido a la intervención; de otra manera probablemente seremos engañados por este truco de regresión a la media.  Además, los participantes deben ser asignados aleatoriamente al grupo control o al experimental, para que no haya ninguna diferencia sistemática entre los grupos (en promedio).

<!-- ### The relation between correlation and regression -->
### La relación entre correlación y regresión

<!-- There is a close relationship between correlation coefficients and regression coefficients.  Remember that Pearson's correlation coefficient is computed as the ratio of the covariance and the product of the standard deviations of x and y: -->
Hay una relación cercana entre los coeficientes de correlación y los coeficientes de regresión. Recuerda que el coeficiente de correlación de Pearson es calculado como la división de la covarianza entre el resultado de la multiplicación de las desviaciones estándar de $x$ y $y$:

$$
\hat{r} = \frac{covarianza_{xy}}{s_x * s_y}
$$
<!-- whereas the regression beta for x is computed as: -->
mientras que el coeficiente de regresión beta para x es calculado como:

$$
\hat{\beta_x} = \frac{covarianza_{xy}}{s_x*s_x}
$$

<!-- Based on these two equations, we can derive the relationship between $\hat{r}$ and $\hat{beta}$: -->
Basándonos en estas dos ecuaciones, podemos derivar la relación entre $\hat{r}$ y $\hat{beta}$:

$$
covariance_{xy} = \hat{r} * s_x * s_y
$$

$$
\hat{\beta_x} =  \frac{\hat{r} * s_x * s_y}{s_x * s_x} = r * \frac{s_y}{s_x}
$$
<!-- That is, the regression slope is equal to the correlation value multiplied by the ratio of standard deviations of y and x.  One thing this tells us is that when the standard deviations of x and y are the same (e.g. when the data have been converted to Z scores), then the correlation estimate is equal to the regression slope estimate. -->
Esto es, la pendiente de la regresión es igual al valor de la correlación multiplicado por el resultado de la división de la desviación estándar de $y$ sobre la desviación estándar de $x$. Una cosa que nos dice esto es que cuando las desviaciones estándar de $x$ y $y$ son iguales (e.g. cuando los datos han sido convertidos a puntajes Z), entonces la correlación estimada es igual a la pendiente estimada de la regresión.

<!-- ### Standard errors for regression models -->
### Errores estándar de los modelos de regresión

<!-- If we want to make inferences about the regression parameter estimates, then we also need an estimate of their variability.  To compute this, we first need to compute the *residual variance* or *error variance* for the model -- that is, how much variability in the dependent variable is not explained by the model.  We can compute the model residuals as follows: -->
Si queremos realizar inferencias acerca de los parámetros estimados de regresión, entonces también necesitamos un estimado de su variabilidad. Para calcular esto, primero necesitamos calcular la *varianza residual* o *varianza de error* del modelo -- esto es, cuánta variabilidad en la variable dependiente no es explicada por el modelo. Podemos calcular los residuales del modelo de la manera siguiente:

$$
residual = y - \hat{y} = y - (x*\hat{\beta_x} + \hat{\beta_0})
$$
<!-- We then compute the *sum of squared errors (SSE)*: -->
Después calculamos la *suma de errores cuadráticos* (*sum of squared errors*, *SSE*):

$$
SS_{error} = \sum_{i=1}^n{(y_i - \hat{y_i})^2} = \sum_{i=1}^n{residuals^2}
$$
<!-- and from this we compute the *mean squared error*: -->
y de aquí podemos calcular la *media del error cuadrático* (*mean squared error*, *MSE*):

$$
MS_{error} = \frac{SS_{error}}{df} = \frac{\sum_{i=1}^n{(y_i - \hat{y_i})^2} }{N - p}
$$
<!-- where the degrees of freedom ($df$) are determined by subtracting the number of estimated parameters (2 in this case: $\hat{\beta_x}$ and $\hat{\beta_0}$) from the number of observations ($N$).  Once we have the mean squared error, we can compute the standard error for the model as: -->
donde los grados de libertad ($gl$, *degrees of freedom*, $df$) se determinan al restar el número de parámetros estimados (2 en este caso: $\hat{\beta_x}$ y $\hat{\beta_0}$) del número de observaciones ($N$).  Una vez que tenemos la media del error cuadrático (MSE), podemos calcular el error estándar del modelo de la siguiente manera:

$$
SE_{model} = \sqrt{MS_{error}}
$$

<!-- In order to get the standard error for a specific regression parameter estimate, $SE_{\beta_x}$, we need to rescale the standard error of the model by the square root of the sum of squares of the X variable: -->
Para poder obtener el error estándar para un parámetro estimado específico de la regresión, $SE_{\beta_x}$, necesitamos reescalar el error estándar del modelo mediante la raíz cuadrada de la suma de los cuadrados de la variable X:

$$
SE_{\hat{\beta}_x} = \frac{SE_{model}}{\sqrt{{\sum{(x_i - \bar{x})^2}}}}
$$

<!-- ### Statistical tests for regression parameters -->
### Pruebas estadísticas para los parámetros de la regresión

<!-- Once we have the parameter estimates and their standard errors, we can compute a *t* statistic to tell us the likelihood of the observed parameter estimates compared to some expected value under the null hypothesis. In this case we will test against the null hypothesis of no effect (i.e. $\beta=0$): -->
Una vez que obtenemos los parámetros estimados y sus errores estándar, podemos calcular un estadístico *t* que nos diga la probabilidad (*likelihood*) del parámetro estimado observado comparado con algún valor esperado bajo la hipótesis nula. En este caso realizaremos la prueba contra la hipótesis nula de que no haya ningún efecto (i.e. $\beta=0$):

$$
\begin{array}{c}
t_{N - p} = \frac{\hat{\beta} - \beta_{expected}}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} - 0}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} }{SE_{\hat{\beta}}}
\end{array}
$$

<!-- In general we would use statistical software to compute these rather than computing them by hand. Here are the results from the linear model function in R: -->
En general usaríamos un software estadístico para calcular estos valores en lugar de calcularlos a mano. Aquí están los resultados de la función del modelo lineal en R:

```{r echo=FALSE}
summary(lmResult)
```

<!-- In this case we see that the intercept is significantly different from zero (which is not very interesting) and that the effect of studyTime on grades is marginally significant (p = .09)  -- the same p-value as the correlation test that we performed earlier. -->
En este caso podemos ver que la constante es significativamente diferente de cero (lo cual no es muy interesante) y que el efecto de *studyTime* sobre las calificaciones es marginalmente significativo (p = .09) -- el mismo valor p que el de la prueba de correlación que realizamos anteriormente.

<!-- ### Quantifying goodness of fit of the model -->
### Cuantificar la bondad de adjuste del modelo

<!-- Sometimes it's useful to quantify how well the model fits the data overall, and one way to do this is to ask how much of the variability in the data is accounted for by the model.  This is quantified using a value called $R^2$ (also known as the *coefficient of determination*).  If there is only one x variable, then this is easy to compute by simply squaring the correlation coefficient: -->
Algunas veces es útil cuantificar qué tan bien ajusta el modelo a los datos en general, y una manera de hacer esto es preguntar cuánta de la variabilidad en los datos es explicada por el modelo. Esto es cuantificado usando un valor llamado $R^2$ (también conocido como *coeficiente de determinación*, o *coefficient of determination* en inglés). Si sólo hay una variable x, entonces este valor es fácil de calcular al simplemente elevar al cuadrado el coeficiente de correlación:

$$
R^2 = r^2
$$
<!-- In the case of our study time example, $R^2$ = `r I(cor(df$studyTime,df$grade)**2)`, which means that we have accounted for about 40% of the variance in grades. -->
En el caso de nuestro ejemplo del tiempo de estudio, $R^2$ = `r I(cor(df$studyTime,df$grade)**2)`, que significa que hemos explicado cerca del 40% de la varianza en las calificaciones.

<!-- More generally we can think of $R^2$ as a measure of the fraction of variance in the data that is accounted for by the model, which can be computed by breaking the variance into multiple components: -->
De manera más general, podemos pensar en $R^2$ como una medida de la fracción de la varianza en nuestros datos que es explicada por el modelo, lo que puede ser calculado fraccionando la varianza en múltiples componentes:

$$
SS_{total} = SS_{modelo} + SS_{error}
$$
<!-- where $SS_{total}$ is the variance of the data ($y$) and $SS_{model}$ and $SS_{error}$ are computed as shown earlier in this chapter.  Using this, we can then compute the coefficient of determination as: -->
donde $SS_{total}$ es la varianza de los datos ($y$) y $SS_{model}$ y $SS_{error}$ son calculadas como se vio previamente en este capítulo. Usando esto, podemos calcular el coeficiente de determinación como:

$$
R^2 = \frac{SS_{modelo}}{SS_{total}} = 1 - \frac{SS_{error}}{SS_{total}}
$$

<!-- A small value of $R^2$ tells us that even if the model fit is statistically significant, it may only explain a small amount of information in the data. -->
Un valor pequeño de $R^2$ nos dice que aún cuando el ajuste del modelo sea estadísticamente significativo, estaría explicando sólo una pequeña cantidad de información en los datos.

<!-- ## Fitting more complex models -->
## Ajustar modelos más complejos

<!-- Often we would like to understand the effects of multiple variables on some particular outcome, and how they relate to one another.  In the context of our study time example, let's say that we discovered that some of the students had previously taken a course on the topic.  If we plot their grades (see Figure  \@ref(fig:LinearRegressionByPriorClass)), we can see that those who had a prior course perform much better than those who had not, given the same amount of study time. We would like to build a statistical model that takes this into account, which we can do by extending the model that we built above: -->
Frecuentemente nos gustaría entender los efectos de múltiples variables sobre un resultado en particular, y cómo se relacionan unas con otras. En el contexto de nuestro ejemplo del tiempo de estudio, digamos que descubrimos que algunos de los estudiantes habían tomado un curso previo sobre el tema (*priorClass*). Si graficamos sus calificaciones (ve la Figura \@ref(fig:LinearRegressionByPriorClass)), podemos ver que aquellos que han tomado un curso previo obtuvieron un rendimiento más alto que aquellos que no, dado un mismo tiempo de estudio. Nos gustaría armar un modelo estadístico que tome eso en cuenta, lo cual podemos hacer expandiendo el modelo que construimos arriba:

$$
\hat{y} = \hat{\beta_1}*studyTime + \hat{\beta_2}*priorClass + \hat{\beta_0}
$$
<!-- To model whether each individual has had a previous class or not, we use what we call *dummy coding* in which we create a new variable that has a value of one to represent having had a class before, and zero otherwise.  This means that for people who have had the class before, we will simply add the value of $\hat{\beta_2}$ to our predicted value for them -- that is, using dummy coding $\hat{\beta_2}$ simply reflects the difference in means between the two groups. Our estimate of $\hat{\beta_1}$ reflects the regression slope over all of the data points --  we are assuming that regression slope is the same regardless of whether someone has had a class before (see Figure  \@ref(fig:LinearRegressionByPriorClass)). -->
Para modelar si cada persona había tomado un curso previo o no, usamos lo que se conoce como *dummy coding* (o *codificación ficticia*) en donde creamos una nueva variable que tiene el valor igual a uno para representar que se ha tomado una clase previa, y un valor de cero si no. Esto significa que para las personas que han tomado una clase previa, simplemente añadiremos el valor $\hat{\beta_2}$ a nuestro valor predicho para ellos -- esto es, el usar *dummy coding* $\hat{\beta_2}$ simplemente refleja la diferencia de las medias de los dos grupos. Nuestra estimación de $\hat{\beta_1}$ refleja la pendiente de regresión a lo largo de todos los datos -- por el momento estamos asumiendo que la pendiente de regresión es la misma sin importar si las personas han tomado una clase previa o no (ve la Figura \@ref(fig:LinearRegressionByPriorClass)).

```{r echo=FALSE}
# perform linear regression for study time and prior class

# must change priorClass to a factor variable
df$priorClass <- as.factor(df$priorClass)

lmResultTwoVars <- lm(grade ~ studyTime + priorClass, data = df)
summary(lmResultTwoVars)

```

<!-- The relation between study time and grade including prior experience as an additional component in the model.  The solid line relates study time to grades for students who have not had prior experience, and the dashed line relates grades to study time for students with prior experience. The dotted line corresponds to the difference in means between the two groups. -->
```{r LinearRegressionByPriorClass,echo=FALSE, fig.cap='La relación entre tiempo de estudio y calificación incluyendo la experiencia previa como un componente adicional en el modelo. La línea sólida relaciona el tiempo de estudio con las calificaciones para los estudiantes que no tienen experiencia previa, y la línea discontinua relaciona las calificaciones con el tiempo de estudio para los estudiantes con experiencia previa. La línea punteada corresponde a la diferencia de las medias de ambos grupos.',fig.width=6,fig.height=4,out.height='50%'}


p <- ggplot(df,aes(studyTime,grade,shape=priorClass)) +
  geom_point(size=3) + xlim(0,5) + ylim(70,100)


p <- p+
  geom_abline(slope=lmResultTwoVars$coefficients[2],
              intercept=lmResultTwoVars$coefficients[1],lineype='dotted')

# p <- p+
#   annotate('segment',x=2,xend=3,
#            y=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            yend=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            color='blue') +
#   annotate('segment',x=3,xend=3,
#            y=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            yend=lmResultTwoVars$coefficients[1]+
#              3*lmResultTwoVars$coefficients[2],
#            color='blue')


p <- p+
  geom_abline(slope=lmResultTwoVars$coefficients[2],
              intercept=lmResultTwoVars$coefficients[1]+
                lmResultTwoVars$coefficients[3],
              linetype='dashed') 

p <- p+
  annotate('segment',x=2,xend=2,
           y=lmResultTwoVars$coefficients[1]+
             2*lmResultTwoVars$coefficients[2],
           yend=lmResultTwoVars$coefficients[1]+
             lmResultTwoVars$coefficients[3] +
             2*lmResultTwoVars$coefficients[2],
           linetype='dotted',size=1) +
  scale_color_discrete(
    limits = c(0, 1),
    labels = c("No", "Yes")
  ) +
  labs(
    color = "Previous course"
  )
print(p)
```

<!-- ## Interactions between variables -->
## Interacciones entre variables

<!-- In the previous model, we assumed that the effect of study time on grade (i.e., the regression slope) was the same for both groups. However, in some cases we might imagine that the effect of one variable might differ depending on the value of another variable, which we refer to as an *interaction* between variables. -->
En el modelo anterior, asumimos que el efecto del tiempo de estudio sobre las calificaciones (i.e. la pendiente de la regresión) era el mismo para ambos grupos. Sin embargo, en algunos casos podríamos imaginar que el efecto de una variable podría diferir dependiendo del valor de alguna otra variable, a lo que llamamos una *interacción* (*interaction*) entre variables.


```{r CaffeineSpeaking, echo=FALSE}
set.seed(1234567)

df <- 
  data.frame(
    group=c(rep(-1,10),
            rep(1,10)
          )
  ) %>%
  mutate(caffeine=runif(n())*100) %>%
  mutate(speaking=0.5*caffeine*-group + group*20 + rnorm(20)*10) %>%
  mutate(anxiety=ifelse(group==1,'anxious','notAnxious'))

```

<!-- Let's use a new example that asks the question: What is the effect of caffeine on public speaking?  First let's generate some data and plot them. -->
Usemos un nuevo ejemplo que haga la pregunta: ¿cuál es el efecto de la cafeína sobre el hablar en público? Primero generemos algunos datos y grafiquémoslos.
<!-- Looking at panel A of Figure \@ref(fig:CaffeineAnxietyInteraction), there doesn't seem to be a relationship, and we can confirm that by performing linear regression on the data: -->
Observando el panel A de la Figura \@ref(fig:CaffeineAnxietyInteraction), no parece haber una relación, y podemos confirmarlo realizando una regresión lineal sobre los datos:

```{r echo=FALSE}
# perform linear regression with caffeine as independent variable
lmResultCaffeine <- lm(speaking ~ caffeine, data = df)
summary(lmResultCaffeine)
```

<!-- But now let's say that we find research suggesting that anxious and non-anxious people react differently to caffeine.  First let's plot the data separately for anxious and non-anxious people. -->
Pero digamos que luego encontramos una investigación que sugiere que las personas ansiosas y las no ansiosas reaccionan diferente a la cafeína. Primero grafiquemos los datos separando a las personas ansiosas de las no ansiosas.

<!-- As we see from panel B in Figure \@ref(fig:CaffeineAnxietyInteraction), it appears that the relationship between speaking and caffeine is different for the two groups, with caffeine improving performance for people without anxiety and degrading performance for those with anxiety.  We'd like to create a statistical model that addresses this question.  First let's see what happens if we just include anxiety in the model. -->
Como podemos ver en el panel B de la Figura \@ref(fig:CaffeineAnxietyInteraction), parece ser que la relación entre el hablar en público y la cafeína es diferente en los dos grupos, donde la cafeína mejora el rendimiento de las personas sin ansiedad y empeora el rendimiento de aquellas que tienen ansiedad. Queremos crear un modelo estadístico que resuelva esta pregunta. Primero veamos qué pasa si incluimos sólo ansiedad en el modelo.

```{r echo=FALSE}
# compute linear regression adding anxiety to model
lmResultCafAnx <- lm(speaking ~ caffeine + anxiety, data = df)
summary(lmResultCafAnx)
```

<!-- Here we see there are no significant effects of either caffeine or anxiety, which might seem a bit confusing.  The problem is that this model is trying to use the same slope relating speaking to caffeine for both groups. If we want to fit them using lines with separate slopes, we need to include an *interaction* in the model, which is equivalent to fitting different lines for each of the two groups; this is often denoted by using the $*$ symbol in the model. -->
Aquí vemos que no hay efectos significativos de la cafeína ni de la ansiedad, lo que podría parecer un poco confuso. El problema es que este modelo está tratando de usar la misma pendiente al relacionar el hablar en público con la cafeína para ambos grupos. Si queremos ajustar los datos usando líneas con pendientes separadas, necesitamos incluir una *interacción* en el modelo, lo que es equivalente a ajustar diferentes líneas para cada uno de los dos grupos; esto es frecuentemente denotado usando el símbolo $*$ en el modelo.

```{r echo=FALSE}
# compute linear regression including caffeine X anxiety interaction
lmResultInteraction <- lm(
  speaking ~ caffeine + anxiety + caffeine * anxiety,
  data = df
)
summary(lmResultInteraction)
```

<!-- From these results we see that there are significant effects of both caffeine and anxiety (which we call *main effects*) and an interaction between caffeine and anxiety. Panel C in Figure \@ref(fig:CaffeineAnxietyInteraction) shows the separate regression lines for each group. -->
De estos resultados podemos ver que hay efectos significativos tanto de la cafeína como de la ansiedad (lo que llamamos *efectos principales*, o *main effects*) y también una interacción entre cafeína y ansiedad. El Panel C de la Figura \@ref(fig:CaffeineAnxietyInteraction) muestra las líneas de regresión separadas para cada grupo.

<!-- A: The relationship between caffeine and public speaking. B: The relationship between caffeine and public speaking, with anxiety represented by the shape of the data points. C: The relationship between public speaking and caffeine, including an interaction with anxiety.  This results in two lines that separately model the slope for each group (dashed for anxious, dotted for non-anxious). -->
```{r CaffeineAnxietyInteraction, echo=FALSE,fig.cap='A: La relación entre cafeína y hablar en público. B: La relación entre cafeína y hablar en público, con la ansiedad representada en la forma de los puntos de los datos. C: La relación entre hablar en público y cafeína, incluyendo una interacción con la ansiedad. Esto resulta en dos líneas que modelan de manera separada la pendiente de cada grupo (discontinua para personas ansiosas, y punteada para personas no-ansiosas).',fig.width=8,fig.height=8,out.width='80%'}

p1 <- ggplot(df,aes(caffeine,speaking)) +
  geom_point()

p2 <- ggplot(df,aes(caffeine,speaking,shape=anxiety)) +
  geom_point() + 
  theme(legend.position = c(0.1, 0.9))

df_anx <- 
  df %>%
  subset(anxiety=='anxious') %>%
  mutate(y=lmResultInteraction$fitted.values[df$anxiety=='anxious'])

df_notanx <- 
  df %>%
  subset(anxiety=='notAnxious')%>%
  mutate(y=lmResultInteraction$fitted.values[df$anxiety=='notAnxious'])



p3 <- ggplot(df,aes(caffeine,speaking,shape=anxiety)) +
   geom_point() + 
   theme(legend.position = c(0.1, 0.9)) +
  geom_line(data=df_anx,
             aes(caffeine,y),linetype='dashed') +
  geom_line(data=df_notanx,
             aes(caffeine,y),linetype='dotted')
plot_grid(p1, p2, p3, labels='AUTO')
```

<!-- One important point to note is that we have to be very careful about interpreting a significant main effect if a significant interaction is also present, since the interaction suggests that the main effect differs according to the values of another variable, and thus is not easily interpretable. -->
Un punto importante por resaltar es que debemos ser muy cuidadosxs al interpretar un efecto principal significativo si también está presente una interacción significativa, porque la interacción sugiere que el efecto principal difiere de acuerdo con los valores de otra variable, por lo que no es fácilmente interpretable.

<!-- Sometimes we want to compare the relative fit of two different models, in order to determine which is a better model; we refer to this as *model comparison*.  For the models above, we can compare the goodness of fit of the model with and without the interaction, using what is called an *analysis of variance*: -->
Algunas veces queremos comparar el ajuste relativo de dos modelos diferentes, para poder determinar cuál modelo es mejor; llamamos a esto *comparación de modelos* (o *model comparison*). Para los modelos anteriores, podemos comparar la bondad de ajuste (*goodness of fit*) del modelo sin y con interacción, usando lo que se conoce como *análisis de varianza* (*analysis of variance*):

```{r echo=FALSE}
anova(lmResultCafAnx, lmResultInteraction)
```

<!-- This tells us that there is good evidence to prefer the model with the interaction over the one without an interaction.  Model comparison is relatively simple in this case because the two models are *nested* -- one of the models is a simplified version of the other model, such that all of the variable in the simpler model are contained in the more complex model.  Model comparison with non-nested models can get much more complicated. -->
Esto nos indica que hay buena evidencia para preferir el modelo que incluye la interacción sobre el modelo que no la incluye. La comparación de modelos es relativamente simple en este caso porque los dos modelos están *anidados* (del inglés: *nested*) -- uno de los modelos es una versión simplificada del otro modelo, de manera tal que todas las variables en el modelo simple están contenidas en el modelo más complejo. La comparación de modelos no-anidados se puede volver mucho más complicada.

<!-- ## Beyond linear predictors and outcomes -->
## Más allá de predictores y resultados lineales

<!-- It is important to note that despite the fact that it is called the general *linear* model, we can actually use the same machinery to model effects that don't follow a straight line (such as curves).  The "linear" in the general linear model doesn't refer to the shape of the response, but instead refers to the fact that model is linear in its parameters --- that is, the predictors in the model only get multiplied the parameters, rather than a nonlinear relationship like being raised to a power of the parameter.  It's also common to analyze data where the outcomes are binary rather than continuous, as we saw in the chapter on categorical outcomes. There are ways to adapt the general linear model (known as *generalized linear models*) that allow this kind of analysis. We will explore these models later in the book. -->
Es importante hacer notar que a pesar del hecho de que se llame modelo *lineal* general, realmente podemos usar la misma maquinaria para modelar efectos que no siguen una línea recta (como las curvas). La palabra "lineal" en el modelo lineal general no se refiere a la forma de la respuesta, sino al hecho de que el modelo es lineal en sus parámetros -- esto es, que los predictores en el modelo sólo pueden ser multiplicados por los parámetros, contrario a relaciones no lineales como el que fueran elevados a la potencia del parámetro. También es común analizar datos donde los resultados son binarios en lugar de continuos, como vimos en el capítulo sobre resultados categóricos. Existen formas de adaptar el modelo lineal general (conocidos como *modelos lineales generalizados*) que permiten este tipo de análisis. Exploraremos estos modelos después en este libro.

<!-- ## Criticizing our model and checking assumptions {#model-criticism} -->
## Criticar nuestro modelo y revisar suposiciones {#model-criticism}

<!-- The saying "garbage in, garbage out" is as true of statistics as anywhere else.  In the case of statistical models, we have to make sure that our model is properly specified and that our data are appropriate for the model. -->
El dicho "si metes basura, sacas basura" ("*garbage in, garbage out*") es cierto para la estadística como en cualquier lugar. En el caso de modelos estadísticos, debemos asegurarnos de que nuestro modelo está especificado apropiadamente y que nuestros datos son apropiados para el modelo.

<!-- When we say that the model is "properly specified", we mean that we have included the appropriate set of independent variables in the model.  We have already seen examples of misspecified models, in Figure \@ref(fig:childHeightLine).  Remember that we saw several cases where the model failed to properly account for the data, such as failing to include an intercept. When building a model, we need to ensure that it includes all of the appropriate variables. -->
Cuando decimos que el modelo está "especificado apropiadamente", queremos decir que hemos incluido el conjunto apropiado de variables independientes en el modelo. Ya hemos visto ejemplos de modelos mal especificados, en la Figura \@ref(fig:childHeightLine).  Recuerda que vimos varios casos donde el modelo falló en explicar apropiadamente los datos, como cuando no se incluyó la constante. Cuando construimos un modelo, debemos asegurarnos que incluye todas las variables apropiadas.

<!-- We also need to worry about whether our model satisfies the assumptions of our statistical methods.  One of the most important assumptions that we make when using the general linear model is that the residuals (that is, the difference between the model's predictions and the actual data) are normally distributed. This can fail for many reasons, either because the model was not properly specified or because the data that we are modeling are inappropriate.  -->
También debemos preocuparnos si nuestro modelo satisface las suposiciones de nuestros métodos estadísticos. Una de las suposiciones más importantes que hacemos cuando usamos el modelo lineal general es que los residuales (esto es, la diferencia entre las predicciones de nuestro modelo y los datos verdaderos) están normalmente distribuidos. Esto puede no cumplirse por varias razones, puede ser porque el modelo no estaba apropiadamente especificado, o porque los datos que estamos modelando son inapropiados.

<!-- We can use something called a *Q-Q* (quantile-quantile) plot to see whether our residuals are normally distributed.  You have already encountered *quantiles* --- they are the value that cuts off a particular proportion of a cumulative distribution. The Q-Q plot presents the quantiles of two distributions against one another; in this case, we will present the quantiles of the actual data against the quantiles of a normal distribution fit to the same data. Figure \@ref(fig:qqplots) shows examples of two such Q-Q plots.  The left panel shows a Q-Q plot for data from a normal distribution, while the right panel shows a Q-Q plot from non-normal data.  The data points in the right panel diverge substantially from the line, reflecting the fact that they are not normally distributed. -->
Podemos usar algo conocido como una gráfica *Q-Q* (*quantile-quantile*, *Q-Q plot*) para ver si nuestros residuales están distribuidos de manera normal. Ya te has encontrado con los *cuantiles* (*quantiles*) --- son los valores que establecen un punto de corte para una proporción particular de una distribución acumulada. La gráfica Q-Q presenta los cuantiles de dos distribuciones una contra la otra; en este caso, presentaremos los cuantiles de los datos reales contra los cuantiles de una distribución normal ajustada a los mismos datos. La Figura \@ref(fig:qqplots) muestra ejemplos de dos gráficas Q-Q. El panel izquierdo muestra una gráfica Q-Q para datos de una distribución normal, mientras que el panel derecho muestra una gráfica Q-Q de una distribución no normal. Los datos en el panel derecho se desvían sustancialmente de la línea diagonal, reflejando el hecho de que no están normalmente distribuidos.

<!-- Q-Q plotsof normal (left) and non-normal (right) data.  The line shows the point at which the x and y axes are equal. -->
```{r qqplots, echo=FALSE, fig.width=8, fig.height=4, out.width="80%", fig.cap="Gráficas Q-Q de datos distribuidos de manera normal (izquierda) y no normal (derecha). La línea diagonal muestra el punto en el que los ejes X y Y serían iguales, es decir, donde deberían caer los datos si se distribuyeran de manera normal."}
qq_df <- tibble(norm=rnorm(100),
                unif=runif(100))

p1 <- ggplot(qq_df,aes(sample=norm)) + 
  geom_qq() + 
  geom_qq_line() + 
  ggtitle('Normal data')

p2 <- ggplot(qq_df,aes(sample=unif)) + 
  geom_qq() + 
  geom_qq_line()+ 
  ggtitle('Non-normal data')

plot_grid(p1,p2)
```

<!-- Model diagnostics will be explored in more detail in a later chapter. -->
Algunas herramientas para el diagnóstico de modelos serán exploradas en mayor detalle en el capítulo siguiente.

<!-- ## What does "predict" really mean? -->
## ¿Qué significa realmente "predecir"?

<!-- When we talk about "prediction" in daily life, we are generally referring to the ability to estimate the value of some variable in advance of seeing the data.  However, the term is often used in the context of linear regression to refer to the fitting of a model to the data; the estimated values ($\hat{y}$) are sometimes referred to as "predictions" and the independent variables are referred to as "predictors".  This has an unfortunate connotation, as it implies that our model should also be able to predict the values of new data points in the future. In reality, the fit of a model to the dataset used to obtain the parameters will nearly always be better than the fit of the model to a new dataset [@copa:1983]. -->
Cuando hablamos de "predicción" en la vida diaria, generalmente nos referimos a la habilidad para estimar el valor de alguna variable antes de ver los datos. Sin embargo, el término frecuentemente es usado en el contexto de regresión lineal para referirse al ajuste de un modelo a los datos; los valores estimados ($\hat{y}$) en ocasiones son conocidos como "predicciones" y las variables independientes son conocidas como "predictores". Esto tiene una connotación desafortunada, porque parece implicar que nuestro modelo debería ser capaz de predecir los valores de nuevos datos en el futuro. En realidad, el ajuste de un modelo al conjunto de datos usado para obtener sus parámetros casi siempre será mejor que el ajuste del mismo modelo a un nuevo conjunto de datos [@copa:1983].

<!-- As an example, let's take a sample of 48 children from NHANES and fit a regression model for weight that includes several regressors (age, height, hours spent watching TV and using the computer, and household income) along with their interactions.   -->
Como ejemplo, tomemos una muestra de 48 niñxs de la base de datos NHANES y ajustemos un modelo de regresión para los datos de peso que incluya varios regresores (edad, altura, horas que pasan viendo TV y usando la computadora, e ingreso económico del hogar) junto con sus interacciones.

```{r echo=FALSE}
# create dataframe with children with complete data on all variables
set.seed(12345)

NHANES_child <-
  NHANES %>%
  drop_na(Height, Weight, TVHrsDayChild, HHIncomeMid, CompHrsDayChild, Age) %>%
  dplyr::filter(Age < 18)

# create function to sample data and compute regression on in-sample and out-of-sample data

get_sample_predictions <- function(sample_size, shuffle = FALSE) {
  # generate a sample from NHANES
  orig_sample <-
    NHANES_child %>%
    sample_n(sample_size)

  # if shuffle is turned on, then randomly shuffle the weight variable
  if (shuffle) {
    orig_sample$Weight <- sample(orig_sample$Weight)
  }
  # compute the regression line for Weight, as a function of several
  # other variables (with all possible interactions between variables)
  heightRegressOrig <- lm(
    Weight ~ Height * TVHrsDayChild * CompHrsDayChild * HHIncomeMid * Age,
    data = orig_sample
  )
  # compute the predictions
  pred_orig <- predict(heightRegressOrig)

  # create a new sample from the same population
  new_sample <-
    NHANES_child %>%
    sample_n(sample_size)
  
  # use the model fom the original sample to predict the 
  # Weight values for the new sample
  pred_new <- predict(heightRegressOrig, new_sample)
  
  # return r-squared and rmse for original and new data
  return(c(
    cor(pred_orig, orig_sample$Weight)**2,
    cor(pred_new, new_sample$Weight)**2,
    sqrt(mean((pred_orig - orig_sample$Weight)**2)),
    sqrt(mean((pred_new - new_sample$Weight)**2))
  ))
}

# implement the function
sim_results <- 
  replicate(100, get_sample_predictions(sample_size = 48, shuffle = FALSE))

sim_results <- 
  t(sim_results) %>%
  data.frame()

mean_rsquared <-
  sim_results %>%
  summarize(
    `RMSE (original data)` = mean(X3),
    `RMSE (new data)` = mean(X4)
  )

# using shuffled y variable to simulate null effect

sim_results <- 
  replicate(100, get_sample_predictions(sample_size = 48, shuffle = TRUE))

sim_results <- 
  t(sim_results) %>%
  data.frame()

mean_rsquared_sim <-
  sim_results %>%
  summarize(
    `RMSE (original data)` = mean(X3),
    `RMSE (new data)` = mean(X4)
  )
combined_rsquared = rbind(mean_rsquared, mean_rsquared_sim) %>%
  mutate(`Data type`=c('True data', 'Shuffled data')) %>%
  dplyr::select(`Data type`,`RMSE (original data)`,`RMSE (new data)`)

# Root mean squared error for model applied to original data and new data, and after shuffling the order of the y variable (in essence making the null hypothesis true)
kable(combined_rsquared, caption='Raíz del error cuadrático medio del modelo ajustado a los datos originales y luego ajustado a nuevos datos, y después de reordenar los valores de la variable Y (en esencia haciendo la hipótesis nula verdadera).')
```


<!-- Here we see that whereas the model fit on the original data showed a very good fit (only off by a few kg per individual), the same model does a much worse job of predicting the weight values for new children sampled from the same population (off by more than 25 kg per individual).  This happens because the model that we specified is quite complex, since it includes not just each of the individual variables, but also all possible combinations of them (i.e. their *interactions*), resulting in a model with 32 parameters.  Since this is almost as many coefficients as there are data points (i.e., the heights of 48 children), the model *overfits* the data, just like the complex polynomial curve in our initial example of overfitting in Section \@ref(overfitting). -->
Aquí podemos ver que mientras que el modelo mostró un ajuste muy bueno en los datos originales (sólo con error de algunos kg por persona), el mismo modelo hace un peor trabajo prediciendo los datos del peso de nuevos niñxs muestreados de la misma población (con un error de más de 25 kg por persona en promedio). Esto sucede porque nuestro modelo especificado es bastante complejo, ya que incluye no sólo las variables individuales, sino también todas las posibles combinaciones entre ellas (i.e. sus *interacciones*), resultando en un modelo con 32 parámetros. Como esto significa tener casi tantos coeficientes como datos en la muestra (i.e., las alturas de 48 niñxs), el modelo se *sobreajusta* (*overfits*) a los datos, justo como sucedió con nuestra curva polinomial compleja en nuestro ejemplo inicial sobre el sobreajuste en la Sección \@ref(overfitting).

<!-- Another way to see the effects of overfitting is to look at what happens if we randomly shuffle the values of the weight variable (shown in the second row of the table). Randomly shuffling the value should make it impossible to predict weight from the other variables, because they should have no systematic relationship.  The results in the table show that even when there is no true relationship to be modeled (because shuffling should have obliterated the relationship), the complex model still shows a very low error in its predictions on the fitted data, because it fits the noise in the specific dataset.  However, when that model is applied to a new dataset, we see that the error is much larger, as it should be. -->
Otra manera de ver los efectos del sobreajuste es el observar qué sucede si reordenamos aleatoriamente los valores de nuestra variable de peso (mostrada en la segunda fila de la tabla). El reordenar aleatoriamente los valores debería hacer imposible el predecir el peso a partir de otras variables, porque no tendrían ningún relación sistemática. Los resultados en la tabla muestran que incluso cuando no hay una verdadera relación a ser modelada (porque el reordenar aleatoriamente debería haber destruido cualquier relación), el modelo complejo aún muestra un error muy bajo en sus predicciones de los datos ajustados, porque termina ajustándose al error en nuestros datos específicos. Sin embargo, cuando el modelo es aplicado a nuevos datos, vemos que el error es mucho mayor, como debería suceder.

<!-- ### Cross-validation {#cross-validation} -->
### Validación cruzada (Cross-validation) {#cross-validation}

<!-- One method that has been developed to help address the problem of overfitting is known as *cross-validation*.  This technique is commonly used within the field of machine learning, which is focused on building models that will generalize well to new data, even when we don't have a new dataset to test the model. The idea behind cross-validation is that we fit our model repeatedly, each time leaving out a subset of the data, and then test the ability of the model to predict the values in each held-out subset. -->
Un método que ha sido desarrollado para ayudar a resolver el problema del sobreajuste es conocido como *validación cruzada* (*cross-validation*). Esta técnica es comúnmente usada en el campo del aprendizaje máquina (*machine learning*), que está enfocado en construir modelos que puedan generalizarse bien a nuevos datos, incluso cuando no tenemos un nuevo conjunto de datos con el cual probar el modelo. La idea detrás de la validación cruzada es que ajustemos nuestro modelo repetidamente, cada vez dejando fuera un subconjunto de los datos, y luego probar la habilidad de nuestro modelo de predecir los valores en cada subconjunto de datos que se dejó fuera.

<!-- A schematic of the  cross-validation procedure. -->
```{r crossvalidation,echo=FALSE,fig.cap="Un esquema del procedimiento de validación cruzada (cross-validation).",fig.height=4,out.height='30%'}
knitr::include_graphics("images/crossvalidation.png")

```

<!-- Let's see how that would work for our weight prediction example.  In this case we will perform 12-fold cross-validation, which means that we will break the data into 12 subsets, and then fit the model 12 times, in each case leaving out one of the subsets and then testing the model's ability to accurately predict the value of the dependent variable for those held-out data points.  Most statistical software provides tools to apply cross-validation to one's data. Using this function we can run cross-validation on 100 samples from the NHANES dataset, and compute the RMSE for cross-validation, along with the RMSE for the original data and a new dataset, as we computed above. -->
Veamos cómo funcionaría en nuestro ejemplo de predicción de pesos. En este caso realizaremos una validación cruzada con 12 subconjuntos (en inglés: *12-fold cross-validation*), lo que significa que dividiremos nuestros datos en 12 subconjuntos, y luego ajustaremos el modelo 12 veces, en cada caso dejando fuera uno de los subconjuntos y luego probando la habilidad del modelo de poder predecir el valor de la variable dependiente para cada uno de los datos que se dejaron fuera. La mayoría de los softwares estadísticos provee herramientas para aplicar validación cruzada. Usando esta función podemos realizar una validación cruzada en 100 muestras de nuestros datos de NHANES, y calcular el RMSE para la validación cruzada, junto con el RMSE para los datos originales y para un nuevo conjunto de datos, como lo hicimos arriba.

```{r, echo=FALSE,warning=FALSE}
# create a function to run cross-validation
# returns the r-squared for the out-of-sample prediction
set.seed(12345)
compute_cv <- function(d, nfolds = 12) {
  # based on https://quantdev.ssri.psu.edu/tutorials/cross-validation-tutorial
  train_ctrl <- trainControl(method = "cv", number = nfolds)
  model_caret <- train(
    Weight ~ Height * TVHrsDayChild * CompHrsDayChild * HHIncomeMid * Age,
    data = d,
    trControl = train_ctrl, # folds
    method = "lm"
  ) # specifying regression model

  r2_cv <- mean(model_caret$resample$Rsquared)
  rmse_cv <- mean(model_caret$resample$RMSE)
  return(c(r2_cv, rmse_cv))
}

# create function to sample data and compute regression on in-sample and out-of-sample data

get_sample_predictions_cv <- function(sample_size, shuffle = FALSE) {
  orig_sample <-
    NHANES_child %>%
    sample_n(sample_size)

  if (shuffle) {
    orig_sample$Weight <- sample(orig_sample$Weight)
  }

  heightRegressOrig <- lm(
    Weight ~ Height * TVHrsDayChild * CompHrsDayChild * HHIncomeMid * Age,
    data = orig_sample
  )

  pred_orig <- predict(heightRegressOrig)

  new_sample <-
    NHANES_child %>%
    sample_n(sample_size)

  pred_new <- predict(heightRegressOrig, new_sample)
  # run crossvalidation on original sample
  cv_output <- compute_cv(orig_sample) #use function created above
  return(c(
    cor(pred_orig, orig_sample$Weight)**2,
    cor(pred_new, new_sample$Weight)**2,
    cv_output[1],
    sqrt(mean((pred_orig - orig_sample$Weight)**2)),
    sqrt(mean((pred_new - new_sample$Weight)**2)),
    cv_output[2]
  ))
}

#implement the function
sim_results <- 
  replicate(1000, get_sample_predictions_cv(sample_size = 48, shuffle = FALSE))

sim_results <- 
  t(sim_results) %>%
  data.frame()

mean_rsquared <-
  sim_results %>%
  summarize(
    `Original data` = mean(X1),
    `New data` = mean(X2),
    `Cross-validation` = mean(X3)
  ) 

mean_rsquared_t = as.data.frame(t(mean_rsquared)) %>%
  rename(`R-squared`=V1)

# R-squared from cross-validation and new data, showing that cross-validation provides a reasonable estimate of the model's performance on new data.
kable(mean_rsquared_t, caption="Coeficiente de determinación (R-squared) de la validación cruzada (cross-validation) y del ajuste a nuevos datos, mostrando que la validación cruzada provee una estimación razonable del rendimiento del modelo sobre nuevos datos.")
```

<!-- Here we see that cross-validation gives us an estimate of predictive accuracy that is much closer to what we see with a completely new dataset than it is to the inflated accuracy that we see with the original dataset -- in fact, it's even slightly more pessimistic than the average for a new dataset, probably because only part of the data are being used to train each of the models.   -->
Aquí vemos que la validación cruzada nos da una estimación de la precisión predictiva que está mucho más cercana a lo que obtenemos con un conjunto de datos completamente nuevo, en lugar de la precisión inflada que vemos con los datos originales -- de hecho, incluso en ocasiones puede ser un poco más pesimista que el promedio para un nuevo conjunto de datos, probablemente porque sólo una parte de los datos está siendo usada para entrenar cada uno de los modelos.

<!-- Note that using cross-validation properly is tricky, and it is recommended that you consult with an expert before using it in practice.  However, this section has hopefully shown you three things: -->
Nota que el usar la validación cruzada apropiadamente es complicado, y es recomendable que consultes con un experto antes de usarlo en la práctica. Sin embargo, esperemos que esta sección te haya mostrado tres cosas:

<!-- - "Prediction" doesn't always mean what you think it means -->
<!-- - Complex models can overfit data very badly, such that one can observe seemingly good prediction even when there is no true signal to predict -->
<!-- - You should view claims about prediction accuracy very skeptically unless they have been done using the appropriate methods. -->
- "Predicción" no siempre significa lo que crees que significa.
- Modelos complejos pueden sobreajustarse a los datos pésimamente, a tal grado que unx podría aparentemente observar una buena predicción incluso cuando no hay una verdadera señal que predecir.
- Deberías de ver de manera muy escéptica afirmaciones acerca de la precisión de predicciones, a menos que hayan sido realizadas usando los métodos apropiados.

<!-- ## Learning objectives -->
## Objetivos de aprendizaje

<!-- Having read this chapter, you should be able to: -->
Habiendo leído este capítulo, deberías ser capaz de:

<!-- * Describe the concept of linear regression and apply it to a dataset -->
<!-- * Describe the concept of the general linear model and provide examples of its application -->
<!-- * Describe how cross-validation can allow us to estimate the predictive performance of a model on new data -->
* Describir el concepto de regresión lineal y aplicarlo a un conjunto de datos.
* Describir el concepto del modelo lineal general y proveer ejemplos de su aplicación.
* Describir cómo se utiliza la validación cruzada (*cross-validation*) para estimar el rendimiento predictivo de un modelo sobre nuevos datos.

<!-- ## Suggested readings -->
## Lecturas sugeridas

<!-- - [The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd Edition)](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) - The "bible" of machine learning methods, available freely online. -->
[The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd Edition)](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) - La "biblia" de los métodos de aprendizaje máquina, disponible en línea de manera libre.

<!-- ## Appendix -->
## Apéndice

<!-- ### Estimating linear regression parameters -->
### Estimar parámetros de una regresión lineal

<!-- We generally estimate the parameters of a linear model from data using *linear algebra*, which is the form of algebra that is applied to vectors and matrices.  If you aren't familiar with linear algebra, don't worry -- you won't actually need to use it here, as R will do all the work for us.  However, a brief excursion in linear algebra can provide some insight into how the model parameters are estimated in practice. -->
Generalmente estimamos los parámetros de un modelo lineal a partir de los datos usando *álgebra lineal*, que es el tipo de álgebra que se aplica a vectores y matrices. Si no estás familiarizadx con el álgebra lineal, no te preocupes -- pues realmente no la necesitaremos usar aquí, porque R (o el programa estadístico que uses) hará el trabajo por nosotrxs. Sin embargo, una breve excursión dentro del álgebra lineal nos puede proveer algún insight a cómo son estimados los parámetros de los modelos en la práctica.

<!-- First, let's introduce the idea of vectors and matrices; you've already encountered them in the context of R, but we will review them here.  A matrix is a set of numbers that are arranged in a square or rectangle, such that there are one or more *dimensions* across which the matrix varies.  It is customary to place different observation units (such as people) in the rows, and different variables in the columns. Let's take our study time data from above. We could arrange these numbers in a matrix, which would have eight rows (one for each student) and two columns (one for study time, and one for grade).  If you are thinking "that sounds like a data frame in R" you are exactly right!  In fact, a data frame is a specialized version of a matrix, and we can convert a data frame to a matrix using the `as.matrix()` function. -->
Primero, presentemos la idea de vectores y matrices; ya los has encontrando en el contexto de R, pero los revisaremos aquí. Una matriz es un conjunto de números que están organizados en un cuadro o rectángulo, de manera que haya una o más *dimensiones* a lo largo de las cuales la matriz varía. La costumbre es colocar las diferentes unidades de observación (como las personas que observamos) en las filas, y las diferentes variables en las columnas. Tomemos nuestro ejemplo anterior sobre el tiempo de estudio. Podríamos organizar nuestros números en una matriz, que tendría ocho renglones o filas (una por cada estudiante) y dos columnas (una para tiempo de estudio, y otra para calificación). Si estás pensando "eso suena como un *dataframe* de R" (o de JASP o Jamovi) ¡estás totalmente en lo correcto! De hecho, un *dataframe* es una versión especializada de una matriz, y podemos convertir un dataframe a una matriz usando la función `as.matrix()`.

```{r}
df <-
  tibble(
    studyTime = c(2, 3, 5, 6, 6, 8, 10, 12) / 3,
    priorClass = c(0, 1, 1, 0, 1, 0, 1, 0)
  ) %>%
  mutate(
    grade = 
      studyTime * betas[1] + 
      priorClass * betas[2] + 
      round(rnorm(8, mean = 70, sd = 5))
  )

df_matrix <- 
  df %>%
  dplyr::select(studyTime, grade) %>%
  as.matrix()
```

<!-- We can write the general linear model in linear algebra as follows: -->
Podemos escribir el modelo lineal general en álgebra lineal de la siguiente manera:

$$
Y = X*\beta + E
$$

<!-- This looks very much like the earlier equation that we used, except that the letters are all capitalized, which is meant to express the fact that they are vectors.   -->
Esto se parece mucho a la ecuación que usamos anteriormente, a excepción de que las letras están todas en mayúsculas, lo que quiere expresar el hecho de que son vectores o matrices.

<!-- We know that the grade data go into the Y matrix, but what goes into the $X$ matrix?  Remember from our initial discussion of linear regression that we need to add a constant in addition to our independent variable of interest, so our $X$ matrix (which we call the *design matrix*) needs to include two columns:  one representing the study time variable, and one column with the same value for each individual (which we generally fill with all ones). We can view the resulting design matrix graphically (see Figure \@ref(fig:GLMmatrix)). -->
Sabemos que los datos de las calificaciones van en la matriz $Y$, pero ¿qué va en la matriz $X$? Recuerda de nuestra discusión inicial sobre la regresión lineal que necesitamos añadir una constante además de nuestra variable independiente de interés, por lo que nuestra matriz $X$ (que llamamos *matriz de diseño*, o *design matrix*) necesita incluir dos columnas: una representando la variable tiempo de estudio, y una columna con el mismo valor para cada persona (que generalmente llenamos con números uno). Podemos ver la matriz de diseño resultante de manera gráfica (ve la Figura \@ref(fig:GLMmatrix)).

<!-- A depiction of the linear model for the study time data in terms of matrix algebra. -->
```{r GLMmatrix, echo=FALSE,fig.cap="Una representación gráfica del modelo lineal del ejemplo de los datos del tiempo de estudio en términos de álgebra de matrices (o álgebra lineal).",fig.width=4,fig.height=3.04}
knitr::include_graphics("images/glm_matrix.png")
```

<!-- The rules of matrix multiplication tell us that the dimensions of the matrices have to match with one another; in this case, the design matrix has dimensions of 8 (rows) X 2 (columns) and the Y variable has dimensions of 8 X 1. Therefore, the $\beta$ matrix needs to have dimensions 2 X 1, since an 8 X 2 matrix multiplied by a 2 X 1 matrix results in an 8 X 1 matrix (as the matching middle dimensions drop out).  The interpretation of the two values in the $\beta$ matrix is that they are the values to be multipled by study time and 1 respectively to obtain the estimated grade for each individual. We can also view the linear model as a set of individual equations for each individual: -->
Las reglas de multiplicación de matrices nos dicen que las dimensiones de las matrices deben coincidir unas con otras; en este caso, la matriz de diseño tiene dimensiones de 8 (filas) X 2 (columnas) y la variable $Y$ tiene dimensiones de 8 X 1. Entonces, la matriz $\beta$ necesita tener dimensiones 2 X 1, porque una matriz 8 X 2 multiplicada por una matriz 2 X 1 resulta en una matriz 8 X 1 (pues las dimensiones de en medio que coinciden terminan desapareciendo). La interpretación de los dos valores en la matriz $\beta$ es que son los valores por los cuales multiplicar el tiempo de estudio y el valor de 1 respectivamente para obtener la estimación de calificación para cada persona en la muestra. También podemos ver el modelo lineal como un conjunto de ecuaciones individuales para cada persona:

$\hat{y}_1 = studyTime_1*\beta_1 + 1*\beta_2$

$\hat{y}_2 = studyTime_2*\beta_1 + 1*\beta_2$

...

$\hat{y}_8 = studyTime_8*\beta_1 + 1*\beta_2$

<!-- Remember that our goal is to determine the best fitting values of $\beta$ given the known values of $X$ and $Y$.  A naive way to do this would be to solve for $\beta$ using simple algebra -- here we drop the error term $E$ because it's out of our control: -->
Recuerda que nuestra meta es el determinar los valores $\beta$ con mejor ajuste dado que conocemos los valores $X$ y $Y$. Una manera ingenua de hacer esto sería el resolver para $\beta$ usando álgebra simple -- aquí quitamos el término para el error $E$ porque está fuera de nuestro control:

$$
\hat{\beta} = \frac{Y}{X}
$$

<!-- The challenge here is that $X$ and $\beta$ are now matrices, not single numbers -- but the rules of linear algebra tell us how to divide by a matrix, which is the same as multiplying by the *inverse* of the matrix (referred to as $X^{-1}$).  We can do this in R: -->
El reto aquí está en que $X$ y $\beta$ son ahora matrices, no números sencillos -- pero las reglas del álgebra lineal nos dicen cómo dividir entre una matriz, que es lo mismo que multiplicarla por el *inverso* de la matriz (referida como $X^{-1}$). Podemos hacer esto en R:

```{r}
# compute beta estimates using linear algebra

#create Y variable 8 x 1 matrix
Y <- as.matrix(df$grade) 
 #create X variable 8 x 2 matrix
X <- matrix(0, nrow = 8, ncol = 2)
#assign studyTime values to first column in X matrix
X[, 1] <- as.matrix(df$studyTime) 
#assign constant of 1 to second column in X matrix
X[, 2] <- 1 

# compute inverse of X using ginv()
# %*% is the R matrix multiplication operator

beta_hat <- ginv(X) %*% Y #multiple the inverse of X by Y
print(beta_hat)
```

<!-- Anyone who is interested in serious use of statistical methods is highly encouraged to invest some time in learning linear algebra, as it provides the basis for nearly all of the tools that are used in standard statistics. -->
Cualquier persona que esté interesada en un uso serio de métodos estadísticos se le recomienda fuertemente que invierta algún tiempo en aprender álgebra lineal, puesto que provee las bases para casi todas las herramientas que son usadas en estadística estándar.

